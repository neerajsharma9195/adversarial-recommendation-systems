{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tianyiyang/git/adversarial-recommendation-systems\n"
     ]
    }
   ],
   "source": [
    "# Change the working directory\n",
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import surprise\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import src.cf.experiment_core as cf_core\n",
    "\n",
    "from src.cf.utils.timer import Timer\n",
    "\n",
    "from src.cf.utils.constants import (\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_ITEM_COL,\n",
    "    DEFAULT_RATING_COL,\n",
    "    DEFAULT_PREDICTION_COL\n",
    ")\n",
    "\n",
    "from src.cf.evaluation.eval_metrics_numba import (\n",
    "    rmse,\n",
    "    mae,\n",
    "    precision_at_k,\n",
    "    recall_at_k\n",
    ")\n",
    "\n",
    "from src.cf.utils.numba_utils import (\n",
    "    getitem_by_row_col\n",
    ")\n",
    "\n",
    "from numba import vectorize, guvectorize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the data... took 6.858623347710818 seconds for loading the dataset.\n",
      "num users total =  62926\n",
      "num cold start users =  35946\n",
      "make train and test sets... `Setup` took 1.815666910726577 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Load data from dataloader \n",
    "masked_R, unmasked_R = cf_core.get_data_from_dataloader()\n",
    "mask = sparse.coo_matrix(cf_core.logical_xor(masked_R, unmasked_R))\n",
    "\n",
    "nnzs = masked_R.getnnz(axis=1)\n",
    "warm_users = nnzs > 2\n",
    "\n",
    "mask_csr = mask.tocsr()\n",
    "unmasked_vals = sparse.coo_matrix(unmasked_R.multiply(mask))\n",
    "unmasked_cold = cf_core.only_cold_start(masked_R, unmasked_vals, warm_users)\n",
    "\n",
    "trainset, testset, cold_testset = cf_core.setup(masked_R, unmasked_vals, unmasked_cold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covert testsets to pd.Dataframe\n",
    "trainset_df = cf_core.surprise_trainset_to_df(trainset)\n",
    "testset_df = cf_core.surprise_testset_to_df(testset)\n",
    "cold_testset_df = cf_core.surprise_testset_to_df(cold_testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 31.745008911006153 seconds for training.\n"
     ]
    }
   ],
   "source": [
    "# Train SVD model\n",
    "svd = surprise.SVD(random_state=0, n_factors=200, n_epochs=30, verbose=False)\n",
    "with Timer() as train_time:\n",
    "    svd.fit(trainset)\n",
    "\n",
    "print(\"Took {} seconds for training.\".format(train_time.interval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(algo: surprise.prediction_algorithms.algo_base.AlgoBase, data: pd.DataFrame, \n",
    "            usercol=DEFAULT_USER_COL, itemcol=DEFAULT_ITEM_COL, predcol=DEFAULT_PREDICTION_COL) -> pd.DataFrame:\n",
    "    \"\"\"Computes predictions of an algorithm from Surprise on the data. Can be used for computing rating metrics like RMSE.\n",
    "    \n",
    "    Args:\n",
    "        algo (surprise.prediction_algorithms.algo_base.AlgoBase): an algorithm from Surprise\n",
    "        data (pd.DataFrame): the data on which to predict\n",
    "        usercol (str): name of the user column\n",
    "        itemcol (str): name of the item column\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: dataframe with usercol, itemcol, predcol\n",
    "    \"\"\"\n",
    "    predictions = [\n",
    "        algo.predict(getattr(row, usercol), getattr(row, itemcol))\n",
    "        for row in data.itertuples()\n",
    "    ]\n",
    "    predictions = pd.DataFrame(predictions)\n",
    "    predictions = predictions.rename(\n",
    "        index=str, columns={\"uid\": usercol, \"iid\": itemcol, \"est\": predcol}\n",
    "    )\n",
    "    return predictions.drop([\"details\", \"r_ui\"], axis=\"columns\")\n",
    "\n",
    "def compute_ranking_predictions(algo: surprise.prediction_algorithms.algo_base.AlgoBase, data: pd.DataFrame, \n",
    "                                usercol=DEFAULT_USER_COL, itemcol=DEFAULT_ITEM_COL, predcol=DEFAULT_PREDICTION_COL,\n",
    "                                remove_seen=False) -> pd.DataFrame:\n",
    "    \"\"\"Computes predictions of an algorithm from Surprise on all users and items in data. It can be used for computing\n",
    "    ranking metrics like NDCG.\n",
    "    \n",
    "    Args:\n",
    "        algo (surprise.prediction_algorithms.algo_base.AlgoBase): an algorithm from Surprise\n",
    "        data (pd.DataFrame): the data from which to get the users and items\n",
    "        usercol (str): name of the user column\n",
    "        itemcol (str): name of the item column\n",
    "        remove_seen (bool): flag to remove (user, item) pairs seen in the training data\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: dataframe with usercol, itemcol, predcol\n",
    "    \"\"\"\n",
    "\n",
    "    iuids = np.array([algo.trainset.to_inner_uid(uid) for uid in data[usercol].unique()])\n",
    "    iiids = np.array([algo.trainset.to_inner_iid(iid) for iid in data[itemcol].unique()])\n",
    "\n",
    "    full_prediction_matrix = algo.pu @ algo.qi.T\n",
    "    full_prediction_matrix += algo.bu.reshape(-1,1)\n",
    "    full_prediction_matrix += algo.bi\n",
    "    full_prediction_matrix += data[DEFAULT_RATING_COL].mean()\n",
    "\n",
    "    full_prediction_matrix = full_prediction_matrix[iuids, :]\n",
    "    full_prediction_matrix = full_prediction_matrix[:, iiids]\n",
    "\n",
    "    return full_prediction_matrix\n",
    "\n",
    "    # userIDs = np.arange(0, full_prediction_matrix.shape[0])\n",
    "    # itemIDs = np.arange(0, full_prediction_matrix.shape[1])\n",
    "    # full_prediction_matrix = full_prediction_matrix.flatten()\n",
    "    \n",
    "\n",
    "    # all_predictions = pd.DataFrame(\n",
    "    #     data=full_prediction_matrix, columns=[usercol, itemcol, predcol]\n",
    "    # )\n",
    "\n",
    "    # if remove_seen:\n",
    "    #     tempdf = pd.concat(\n",
    "    #         [\n",
    "    #             data[[usercol, itemcol]],\n",
    "    #             pd.DataFrame(\n",
    "    #                 data=np.ones(data.shape[0]), columns=[\"dummycol\"], index=data.index\n",
    "    #             ),\n",
    "    #         ],\n",
    "    #         axis=1,\n",
    "    #     )\n",
    "    #     merged = pd.merge(tempdf, all_predictions, on=[usercol, itemcol], how=\"outer\")\n",
    "    #     return merged[merged[\"dummycol\"].isnull()].drop(\"dummycol\", axis=1)\n",
    "    # else:\n",
    "    #     return all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(svd, testset_df, usercol='userID', itemcol='itemID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>userID</th>\n      <th>itemID</th>\n      <th>prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>240</td>\n      <td>4.414653</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>3258</td>\n      <td>4.391092</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>2954</td>\n      <td>4.414337</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>4178</td>\n      <td>4.273617</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>6860</td>\n      <td>4.849997</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   userID  itemID  prediction\n0       0     240    4.414653\n1       0    3258    4.391092\n2       1    2954    4.414337\n3       1    4178    4.273617\n4       1    6860    4.849997"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 27.25401992024854 seconds for prediction.\n"
     ]
    }
   ],
   "source": [
    "with Timer() as test_time:\n",
    "    all_predictions = compute_ranking_predictions(\n",
    "        svd, trainset_df, usercol='userID', itemcol='itemID', remove_seen=True\n",
    "    )\n",
    "    \n",
    "print(\"Took {} seconds for prediction.\".format(test_time.interval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_array = masked_R.toarray()\n",
    "all_array = all_array[:, np.sum(all_array, axis=0) != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>userID</th>\n      <th>itemID</th>\n      <th>prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>240</td>\n      <td>4.414653</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>3258</td>\n      <td>4.391092</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>2954</td>\n      <td>4.414337</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>4178</td>\n      <td>4.273617</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>6860</td>\n      <td>4.849997</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>187704</th>\n      <td>62925</td>\n      <td>3930</td>\n      <td>4.162117</td>\n    </tr>\n    <tr>\n      <th>187705</th>\n      <td>62925</td>\n      <td>3948</td>\n      <td>4.980985</td>\n    </tr>\n    <tr>\n      <th>187706</th>\n      <td>62925</td>\n      <td>4031</td>\n      <td>4.930572</td>\n    </tr>\n    <tr>\n      <th>187707</th>\n      <td>62925</td>\n      <td>4470</td>\n      <td>4.896207</td>\n    </tr>\n    <tr>\n      <th>187708</th>\n      <td>62925</td>\n      <td>6547</td>\n      <td>4.467122</td>\n    </tr>\n  </tbody>\n</table>\n<p>187709 rows × 3 columns</p>\n</div>",
      "text/plain": "        userID  itemID  prediction\n0            0     240    4.414653\n1            0    3258    4.391092\n2            1    2954    4.414337\n3            1    4178    4.273617\n4            1    6860    4.849997\n...        ...     ...         ...\n187704   62925    3930    4.162117\n187705   62925    3948    4.980985\n187706   62925    4031    4.930572\n187707   62925    4470    4.896207\n187708   62925    6547    4.467122\n\n[187709 rows x 3 columns]"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE and MAE\n",
    "eval_rmse = rmse(testset_df, predictions)\n",
    "eval_mae = mae(testset_df, predictions)\n",
    "\n",
    "print(\"RMSE:\\t\\t%f\" % eval_rmse,\n",
    "      \"MAE:\\t\\t%f\" % eval_mae, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP:\t0.430784\n",
      "Precision@K:\t1.000000\n",
      "Recall@K:\t0.430784\n",
      "MAP:\t0.704941\n",
      "Precision@K:\t0.921687\n",
      "Recall@K:\t0.704941\n",
      "MAP:\t0.851562\n",
      "Precision@K:\t0.810558\n",
      "Recall@K:\t0.851562\n",
      "MAP:\t0.919797\n",
      "Precision@K:\t0.696204\n",
      "Recall@K:\t0.919797\n",
      "MAP:\t0.952551\n",
      "Precision@K:\t0.599207\n",
      "Recall@K:\t0.952551\n",
      "MAP:\t0.969857\n",
      "Precision@K:\t0.521669\n",
      "Recall@K:\t0.969857\n",
      "MAP:\t0.979925\n",
      "Precision@K:\t0.460081\n",
      "Recall@K:\t0.979925\n",
      "MAP:\t0.986045\n",
      "Precision@K:\t0.410435\n",
      "Recall@K:\t0.986045\n",
      "MAP:\t0.989967\n",
      "Precision@K:\t0.369868\n",
      "Recall@K:\t0.989967\n"
     ]
    }
   ],
   "source": [
    "# k = 5\n",
    "\n",
    "for k in range(1,10):\n",
    "      eval_map = map_at_k(trainset_df, all_predictions, col_prediction='prediction', k=k)\n",
    "      # eval_ndcg = ndcg_at_k(testset_df, predictions, col_prediction='prediction', k=k)\n",
    "      eval_precision = precision_at_k(trainset_df, all_predictions, col_prediction='prediction', k=k)\n",
    "      eval_recall = recall_at_k(trainset_df, all_predictions, col_prediction='prediction', k=k)\n",
    "\n",
    "      print(\"MAP:\\t%f\" % eval_map,\n",
    "            # \"NDCG:\\t%f\" % eval_ndcg,\n",
    "            \"Precision@K:\\t%f\" % eval_precision,\n",
    "            \"Recall@K:\\t%f\" % eval_recall, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = sparse.coo_matrix(all_array)\n",
    "uid, iid = tmp.row, tmp.col\n",
    "all_predictions[uid, iid] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = unmasked_R.toarray()\n",
    "ground_truth = ground_truth[:, np.sum(masked_R.toarray(), axis=0) != 0]\n",
    "ground_truth[uid, iid] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_coo = sparse.coo_matrix(ground_truth)\n",
    "partial_all_predictions_coo = sparse.coo_matrix((all_predictions[ground_truth > 0], (tmp_coo.row, tmp_coo.col)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@K:\t0.000397\n",
      "Recall@K:\t0.000131\n",
      "\n",
      "Precision@K:\t0.000373\n",
      "Recall@K:\t0.000219\n",
      "\n",
      "Precision@K:\t0.000350\n",
      "Recall@K:\t0.000324\n",
      "\n",
      "Precision@K:\t0.000334\n",
      "Recall@K:\t0.000430\n",
      "\n",
      "Precision@K:\t0.000308\n",
      "Recall@K:\t0.000528\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# k = 5\n",
    "\n",
    "for k in range(1,6):\n",
    "    eval_precision = precision_at_k(ground_truth, all_predictions, relevancy_method='top_k', k=k, threshold=3.5)\n",
    "    eval_recall = recall_at_k(ground_truth, all_predictions, relevancy_method='top_k', k=k, threshold=3.5)\n",
    "\n",
    "    print(\"Precision@K:\\t%f\" % eval_precision,\n",
    "        \"Recall@K:\\t%f\" % eval_recall, sep='\\n')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('696ds': conda)",
   "name": "python392jvsc74a57bd0ace4d44eac2e7e40a0d49a39d0327809e34998babf341fa7b89e7f058bea5b85"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "ace4d44eac2e7e40a0d49a39d0327809e34998babf341fa7b89e7f058bea5b85"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}