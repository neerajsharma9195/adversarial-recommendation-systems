{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tianyiyang/git/adversarial-recommendation-systems\n"
     ]
    }
   ],
   "source": [
    "# Change the working directory\n",
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import surprise\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import src.cf.experiment_core as cf_core\n",
    "\n",
    "from src.cf.utils.timer import Timer\n",
    "\n",
    "from src.cf.utils.constants import (\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_ITEM_COL,\n",
    "    DEFAULT_PREDICTION_COL\n",
    ")\n",
    "\n",
    "from src.cf.evaluation.eval_metrics import (\n",
    "    rmse,\n",
    "    mae,\n",
    "    map_at_k,\n",
    "    ndcg_at_k,\n",
    "    precision_at_k,\n",
    "    recall_at_k\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the data... took 7.065692531876266 seconds for loading the dataset.\n",
      "num users total =  62926\n",
      "num cold start users =  35946\n",
      "make train and test sets... `Setup` took 1.1733244736678898 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Load data from dataloader \n",
    "masked_R, unmasked_R = cf_core.get_data_from_dataloader()\n",
    "\n",
    "mask = sparse.coo_matrix(cf_core.logical_xor(masked_R, unmasked_R))\n",
    "mask_csr = mask.tocsr()\n",
    "unmasked_vals = sparse.coo_matrix(unmasked_R.multiply(mask))\n",
    "unmasked_cold = cf_core.only_cold_start(masked_R, unmasked_vals)\n",
    "\n",
    "trainset, testset, cold_testset = cf_core.setup(masked_R, unmasked_vals, unmasked_cold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covert testsets to pd.Dataframe\n",
    "trainset_df = cf_core.surprise_trainset_to_df(trainset)\n",
    "testset_df = cf_core.surprise_testset_to_df(testset)\n",
    "cold_testset_df = cf_core.surprise_testset_to_df(cold_testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing epoch 0\n",
      "Processing epoch 1\n",
      "Processing epoch 2\n",
      "Processing epoch 3\n",
      "Processing epoch 4\n",
      "Processing epoch 5\n",
      "Processing epoch 6\n",
      "Processing epoch 7\n",
      "Processing epoch 8\n",
      "Processing epoch 9\n",
      "Processing epoch 10\n",
      "Processing epoch 11\n",
      "Processing epoch 12\n",
      "Processing epoch 13\n",
      "Processing epoch 14\n",
      "Processing epoch 15\n",
      "Processing epoch 16\n",
      "Processing epoch 17\n",
      "Processing epoch 18\n",
      "Processing epoch 19\n",
      "Processing epoch 20\n",
      "Processing epoch 21\n",
      "Processing epoch 22\n",
      "Processing epoch 23\n",
      "Processing epoch 24\n",
      "Processing epoch 25\n",
      "Processing epoch 26\n",
      "Processing epoch 27\n",
      "Processing epoch 28\n",
      "Processing epoch 29\n",
      "Took 30.220278419088572 seconds for training.\n"
     ]
    }
   ],
   "source": [
    "# Train SVD model\n",
    "svd = surprise.SVD(random_state=0, n_factors=200, n_epochs=30, verbose=True)\n",
    "with Timer() as train_time:\n",
    "    svd.fit(trainset)\n",
    "\n",
    "print(\"Took {} seconds for training.\".format(train_time.interval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(algo: surprise.prediction_algorithms.algo_base.AlgoBase, data: pd.DataFrame, \n",
    "            usercol=DEFAULT_USER_COL, itemcol=DEFAULT_ITEM_COL, predcol=DEFAULT_PREDICTION_COL) -> pd.DataFrame:\n",
    "    \"\"\"Computes predictions of an algorithm from Surprise on the data. Can be used for computing rating metrics like RMSE.\n",
    "    \n",
    "    Args:\n",
    "        algo (surprise.prediction_algorithms.algo_base.AlgoBase): an algorithm from Surprise\n",
    "        data (pd.DataFrame): the data on which to predict\n",
    "        usercol (str): name of the user column\n",
    "        itemcol (str): name of the item column\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: dataframe with usercol, itemcol, predcol\n",
    "    \"\"\"\n",
    "    predictions = [\n",
    "        algo.predict(getattr(row, usercol), getattr(row, itemcol))\n",
    "        for row in data.itertuples()\n",
    "    ]\n",
    "    predictions = pd.DataFrame(predictions)\n",
    "    predictions = predictions.rename(\n",
    "        index=str, columns={\"uid\": usercol, \"iid\": itemcol, \"est\": predcol}\n",
    "    )\n",
    "    return predictions.drop([\"details\", \"r_ui\"], axis=\"columns\")\n",
    "\n",
    "def compute_ranking_predictions(algo: surprise.prediction_algorithms.algo_base.AlgoBase, data: pd.DataFrame, \n",
    "                                usercol=DEFAULT_USER_COL, itemcol=DEFAULT_ITEM_COL, predcol=DEFAULT_PREDICTION_COL,\n",
    "                                remove_seen=False) -> pd.DataFrame:\n",
    "    \"\"\"Computes predictions of an algorithm from Surprise on all users and items in data. It can be used for computing\n",
    "    ranking metrics like NDCG.\n",
    "    \n",
    "    Args:\n",
    "        algo (surprise.prediction_algorithms.algo_base.AlgoBase): an algorithm from Surprise\n",
    "        data (pd.DataFrame): the data from which to get the users and items\n",
    "        usercol (str): name of the user column\n",
    "        itemcol (str): name of the item column\n",
    "        remove_seen (bool): flag to remove (user, item) pairs seen in the training data\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: dataframe with usercol, itemcol, predcol\n",
    "    \"\"\"\n",
    "    preds_lst = []\n",
    "    users = data[usercol].unique()\n",
    "    items = data[itemcol].unique()\n",
    "\n",
    "    for user in users:\n",
    "        for item in items:\n",
    "            preds_lst.append([user, item, algo.predict(user, item).est])\n",
    "\n",
    "    all_predictions = pd.DataFrame(data=preds_lst, columns=[usercol, itemcol, predcol])\n",
    "\n",
    "    if remove_seen:\n",
    "        tempdf = pd.concat(\n",
    "            [\n",
    "                data[[usercol, itemcol]],\n",
    "                pd.DataFrame(\n",
    "                    data=np.ones(data.shape[0]), columns=[\"dummycol\"], index=data.index\n",
    "                ),\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "        merged = pd.merge(tempdf, all_predictions, on=[usercol, itemcol], how=\"outer\")\n",
    "        return merged[merged[\"dummycol\"].isnull()].drop(\"dummycol\", axis=1)\n",
    "    else:\n",
    "        return all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(svd, testset_df, usercol='userID', itemcol='itemID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>userID</th>\n      <th>itemID</th>\n      <th>prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>240</td>\n      <td>4.414653</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>3258</td>\n      <td>4.391092</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>2954</td>\n      <td>4.414337</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>4178</td>\n      <td>4.273617</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>6860</td>\n      <td>4.849997</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   userID  itemID  prediction\n0       0     240    4.414653\n1       0    3258    4.391092\n2       1    2954    4.414337\n3       1    4178    4.273617\n4       1    6860    4.849997"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with Timer() as test_time:\n",
    "#     all_predictions = compute_ranking_predictions(\n",
    "#         svd, testset_df, usercol='userID', itemcol='itemID', remove_seen=True\n",
    "#     )\n",
    "    \n",
    "# print(\"Took {} seconds for prediction.\".format(test_time.interval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# all_predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:\t\t0.974254\n",
      "MAE:\t\t0.678489\n"
     ]
    }
   ],
   "source": [
    "# RMSE and MAE\n",
    "eval_rmse = rmse(testset_df, predictions)\n",
    "eval_mae = mae(testset_df, predictions)\n",
    "\n",
    "print(\"RMSE:\\t\\t%f\" % eval_rmse,\n",
    "      \"MAE:\\t\\t%f\" % eval_mae, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP:\t0.430784\n",
      "Precision@K:\t1.000000\n",
      "Recall@K:\t0.430784\n",
      "MAP:\t0.704941\n",
      "Precision@K:\t0.921687\n",
      "Recall@K:\t0.704941\n",
      "MAP:\t0.851562\n",
      "Precision@K:\t0.810558\n",
      "Recall@K:\t0.851562\n",
      "MAP:\t0.919797\n",
      "Precision@K:\t0.696204\n",
      "Recall@K:\t0.919797\n",
      "MAP:\t0.952551\n",
      "Precision@K:\t0.599207\n",
      "Recall@K:\t0.952551\n",
      "MAP:\t0.969857\n",
      "Precision@K:\t0.521669\n",
      "Recall@K:\t0.969857\n",
      "MAP:\t0.979925\n",
      "Precision@K:\t0.460081\n",
      "Recall@K:\t0.979925\n",
      "MAP:\t0.986045\n",
      "Precision@K:\t0.410435\n",
      "Recall@K:\t0.986045\n",
      "MAP:\t0.989967\n",
      "Precision@K:\t0.369868\n",
      "Recall@K:\t0.989967\n"
     ]
    }
   ],
   "source": [
    "# k = 5\n",
    "\n",
    "for k in range(1,10):\n",
    "      eval_map = map_at_k(testset_df, predictions, col_prediction='prediction', k=k)\n",
    "      # eval_ndcg = ndcg_at_k(testset_df, predictions, col_prediction='prediction', k=k)\n",
    "      eval_precision = precision_at_k(testset_df, predictions, col_prediction='prediction', k=k)\n",
    "      eval_recall = recall_at_k(testset_df, predictions, col_prediction='prediction', k=k)\n",
    "\n",
    "      print(\"MAP:\\t%f\" % eval_map,\n",
    "            # \"NDCG:\\t%f\" % eval_ndcg,\n",
    "            \"Precision@K:\\t%f\" % eval_precision,\n",
    "            \"Recall@K:\\t%f\" % eval_recall, sep='\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('696ds': conda)",
   "name": "python392jvsc74a57bd0ace4d44eac2e7e40a0d49a39d0327809e34998babf341fa7b89e7f058bea5b85"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "ace4d44eac2e7e40a0d49a39d0327809e34998babf341fa7b89e7f058bea5b85"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}